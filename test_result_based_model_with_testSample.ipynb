{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffe0bfa-96af-48e3-bb26-e49837f350e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: rouge_score in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: bert_score in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: absl-py in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bert_score) (2.9.1)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bert_score) (4.56.2)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bert_score) (3.8.2)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.0.0->bert_score) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers>=3.0.0->bert_score) (0.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib->bert_score) (3.2.5)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk->rouge_score) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba2969a-bcc8-49f2-aeae-25b2f57dd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abce6655-2327-41aa-ac69-34fa7a3555fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b24ae41-c868-4820-8169-6259421aafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable TF32 on A100 (big speed-up, minimal quality impact)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a66add0-4e09-4cab-ab4e-0505490f8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee7b63f-ea7a-42ec-bea0-bf6d967a985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (2-27): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_16bit = True     # fp16 is fine for A100\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_16bit=load_in_16bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799a2e1-ad46-4233-b40c-30600770a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 Dec 2025\n",
      "\n",
      "user\n",
      "\n",
      "What is Pulmonary fibrosis?assistant\n",
      "\n",
      "Pulmonary fibrosis (PF) is a chronic and progressive lung disease characterized by scarring and thickening of the lung tissue, leading to impaired lung function and breathing difficulties. The scarring can be caused by various factors, including:\n",
      "\n",
      "1. **Idiopathic pulmonary fibrosis (IPF)**: A condition where the exact cause is unknown, but it is believed to be related to genetic and environmental factors.\n",
      "2. **Exposure to certain substances**: Such as asbestos, silica, and certain chemicals, which can cause inflammation and scarring in the lungs.\n",
      "3. **Infections**: Such as pneumonia, tuberculosis, and histoplasmosis, which can cause lung damage and scarring.\n",
      "4. **Autoimmune disorders**: Such as rheumatoid arthritis and lupus, which can cause inflammation and scarring in the lungs.\n",
      "5. **Genetic disorders**: Such as alpha-1 antitrypsin deficiency, which can cause lung damage and scarring.\n",
      "\n",
      "Symptoms\n"
     ]
    }
   ],
   "source": [
    "# Test a simple prompt\n",
    "prompt = \"What is Pulmonary fibrosis?\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee0db8-01df-4b16-8519-9fa67816a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 996\n",
      "Evaluating 996 samples...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"mixed_test_1000_no_resources.csv\"   \n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "dataset = dataset.rename(columns={\"instruction\": \"question\", \"output\": \"answer\"})\n",
    "print(f\"Total samples loaded: {len(dataset)}\")\n",
    "\n",
    "num_eval = min(1000, len(dataset))\n",
    "print(f\"Evaluating {num_eval} samples...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deca6b4-c3cd-4d61-9878-c15b71ad2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return (text.replace(\"Assistant:\", \"\").replace(\"\\n\", \" \").strip())\n",
    "\n",
    "def clean_simple(text):\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"assistant:\", \"\").replace(\"assistant\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text.strip()\n",
    "\n",
    "def overlap_ratio(pred, ref):\n",
    "    pw, rw = set(pred.split()), set(ref.split())\n",
    "    return len(pw & rw) / len(pw) if pw else 0\n",
    "\n",
    "def sim_ratio(pred, ref):\n",
    "    return SequenceMatcher(None, pred, ref).ratio()\n",
    "\n",
    "\n",
    "# Batched Generation\n",
    "\n",
    "def generate_responses_batched(questions, batch_size=16, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Generate deterministic answers for a list of questions using batching.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "\n",
    "    for start in tqdm(range(0, len(questions), batch_size), desc=\"Generating\"):\n",
    "        batch_q = questions[start:start + batch_size]\n",
    "        # Build list of chat conversations\n",
    "        messages = [[{\"role\": \"user\", \"content\": q}] for q in batch_q]\n",
    "\n",
    "        # Tokenize with padding for batch\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,   # deterministic\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        # Decode each sample\n",
    "        for out_ids, q in zip(outputs, batch_q):\n",
    "            decoded = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "            # Heuristic: split on question to get answer part\n",
    "            answer = decoded.split(q)[-1].strip()\n",
    "            all_predictions.append(clean_text(answer))\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413064d-e40c-4f56-8324-db26e4ef5820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [09:25<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= Base Model Evaluation =================\n",
      "Test Samples: 996\n",
      "ROUGE-L Score: 0.0814\n",
      "BERTScore F1: 0.0252\n",
      "==========================================================\n",
      "\n",
      "Saved â†’ based_model_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Prediction Loop (Batched)\n",
    "\n",
    "questions = list(dataset[\"question\"].head(num_eval))\n",
    "references = [clean_text(a) for a in dataset[\"answer\"].head(num_eval)]\n",
    "\n",
    "predictions = generate_responses_batched(\n",
    "    questions,\n",
    "    batch_size=16,        \n",
    "    max_new_tokens=256    \n",
    ")\n",
    "\n",
    "assert len(predictions) == len(references) == num_eval\n",
    "\n",
    "\n",
    "# Metrics\n",
    "\n",
    "print(\"\\nCalculating metrics...\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# BERTScore on GPU, batched\n",
    "bert_results = bertscore.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    lang=\"en\",\n",
    "    rescale_with_baseline=True,\n",
    "    device=device,\n",
    "    batch_size=32,    \n",
    ")\n",
    "\n",
    "bert_f1_mean = float(np.mean(bert_results[\"f1\"]))\n",
    "\n",
    "\n",
    "print(\"\\n================= Base Model Evaluation =================\")\n",
    "print(f\"Test Samples: {num_eval}\")\n",
    "print(f\"ROUGE-L Score: {rouge_results['rougeL']:.4f}\")\n",
    "print(f\"BERTScore F1: {bert_f1_mean:.4f}\")\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Question\": questions,\n",
    "    \"Ground_Truth\": references,\n",
    "    \"Model_Prediction\": predictions\n",
    "})\n",
    "df.to_csv(\"based_model_eval_results.csv\", index=False)\n",
    "print(\"Saved â†’ based_model_eval_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55193de-5988-4fca-a382-f25d98fb78b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece09a6-ec33-419a-9934-6da31994baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Mean Precision: -0.2271\n",
      "BERTScore Mean Recall: 0.3041\n",
      "BERTScore Mean F1: 0.0252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Calculate mean F1, recall, and precision\n",
    "mean_precision = torch.mean(torch.tensor(bert_results['precision'])).item()\n",
    "mean_recall = torch.mean(torch.tensor(bert_results['recall'])).item()\n",
    "mean_f1 = torch.mean(torch.tensor(bert_results['f1'])).item()\n",
    "\n",
    "print(f\"BERTScore Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"BERTScore Mean Recall: {mean_recall:.4f}\")\n",
    "print(f\"BERTScore Mean F1: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e91f7-5740-4b45-b430-cc2d0d2c3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved â†’ based_model_eval_with_hallucinations.csv\n",
      "\n",
      "Hallucination Rate: 0.9979919678714859\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Hallucination Detection\n",
    "\n",
    "hallucinations = []\n",
    "overlaps, sim_scores = [], []\n",
    "\n",
    "for p, r in zip(predictions, references):\n",
    "    cp, cr = clean_simple(p), clean_simple(r)\n",
    "\n",
    "    ov = overlap_ratio(cp, cr)\n",
    "    sr = sim_ratio(cp, cr)\n",
    "\n",
    "    overlaps.append(ov)\n",
    "    sim_scores.append(sr)\n",
    "\n",
    "    hallucinations.append(\n",
    "        len(cp.split()) > 10 and ov < 0.25 and sr < 0.40\n",
    "    )\n",
    "\n",
    "df[\"overlap_ratio\"] = overlaps\n",
    "df[\"similarity\"] = sim_scores\n",
    "df[\"hallucination_flag\"] = hallucinations\n",
    "\n",
    "df.to_csv(\"based_model_eval_with_hallucinations.csv\", index=False)\n",
    "print(\"Saved â†’ based_model_eval_with_hallucinations.csv\")\n",
    "\n",
    "print(\"\\nHallucination Rate:\", df[\"hallucination_flag\"].mean())\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
