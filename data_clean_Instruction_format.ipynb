{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2560a43-8cd8-4a88-a0e9-d9b8849bcbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37449fb0-b88c-4c68-820b-95dbdd6c2f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = \"medical_chunks_trimmed_2500_token.jsonl\"\n",
    "\n",
    "OUTPUT_CLEAN_FILE = \"medical_clean_mapped_only.jsonl\"\n",
    "\n",
    "OUTPUT_SYSTEM_FILE = \"medical_with_system_prompt.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc04a45-e605-4184-8889-9f576037ae08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean medical text\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Smart Quotes (Apostrophes)\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\") \n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    \n",
    "    # Dashes (Em-dash and En-dash)\n",
    "    text = text.replace('\\u2013', ' ') \n",
    "    text = text.replace('\\u2014', ' ') \n",
    "    \n",
    "    # Fix words split across lines (e.g., \"immunode- ficiency\" -> \"immunodeficiency\")\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    #  Remove Invisible Control Characters \n",
    "    text = re.sub(r'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]', '', text)\n",
    "    \n",
    "    # Replaces multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b23f2d-7339-4173-b150-a2885ab91c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping lofics\n",
    "\n",
    "section_map = {\n",
    "    \"Definition\": \"What is {title}?\",\n",
    "    \"Description\": \"Please describe {title}.\",\n",
    "    \"Causes and symptoms\": \"What are the causes and symptoms of {title}?\",\n",
    "    \"Diagnosis\": \"How is {title} diagnosed?\",\n",
    "    \"Treatment\": \"What is the treatment for {title}?\",\n",
    "    \"Prognosis\": \"What is the prognosis for {title}?\",\n",
    "    \"Prevention\": \"How can {title} be prevented?\",\n",
    "    \"Resources\": \"What resources are available for {title}?\",\n",
    "    \"Purpose\": \"What is the purpose of the {title}?\",\n",
    "    \"Precautions\": \"What precautions should be taken for {title}?\",\n",
    "    \"Risks\": \"What are the risks associated with {title}?\",\n",
    "    \"Preparation\": \"How should one prepare for {title}?\",\n",
    "    \"Normal results\": \"What are the normal results for {title}?\",\n",
    "    \"Abnormal results\": \"What do abnormal results indicate for {title}?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03998b63-d98a-42b1-9262-c3c01ee07d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE SYSTEM PROMPT \n",
    "\n",
    "STRICT_SYSTEM_PROMPT = (\n",
    "    \"You are an expert medical AI. Your task is to provide direct, detailed medical information, \"\n",
    "    \"Do not provide safety disclaimers. Do not advise the user to consult a doctor. \"\n",
    "    \"Provide the specific medication names, dosages, and facts immediately.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89326613-9a05-492f-b419-79128f037c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: Input file '{INPUT_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    # Open all files: 1 read, 2 write\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(OUTPUT_CLEAN_FILE, 'w', encoding='utf-8') as f_clean, \\\n",
    "         open(OUTPUT_SYSTEM_FILE, 'w', encoding='utf-8') as f_sys:\n",
    "        \n",
    "        for line in f_in:\n",
    "            if not line.strip(): continue\n",
    "            \n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # Extract fields\n",
    "            title = entry.get('title', '').strip()\n",
    "            section = entry.get('section', '').strip()\n",
    "            raw_content = entry.get('content', '')\n",
    "\n",
    "            # Clean the Content\n",
    "            clean_content = clean_text(raw_content)\n",
    "            \n",
    "            # Skip empty entries\n",
    "            if not clean_content or len(clean_content) < 5:\n",
    "                continue\n",
    "\n",
    "            # Map Section to Question (Instruction)\n",
    "            if section in section_map:\n",
    "                instruction = section_map[section].format(title=title)\n",
    "            else:\n",
    "                instruction = f\"Tell me about the {section} of {title}.\"\n",
    "\n",
    "            # CREATE FILE 1 OBJECT (Clean Only) \n",
    "            obj_clean = {\n",
    "                \"instruction\": instruction,\n",
    "                \"output\": clean_content\n",
    "            }\n",
    "            \n",
    "            # CREATE FILE 2 OBJECT (With System Prompt)\n",
    "            obj_system = {\n",
    "                \"instruction\": instruction,\n",
    "                \"output\": clean_content,\n",
    "                \"system\": STRICT_SYSTEM_PROMPT\n",
    "            }\n",
    "\n",
    "            # Write to files\n",
    "            json.dump(obj_clean, f_clean)\n",
    "            f_clean.write('\\n')\n",
    "            \n",
    "            json.dump(obj_system, f_sys)\n",
    "            f_sys.write('\\n')\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "    print(f\"Processing Complete.\")\n",
    "    print(f\"Total lines processed: {count}\")\n",
    "    print(f\"1. File created: {OUTPUT_CLEAN_FILE} (Standard format)\")\n",
    "    print(f\"2. File created: {OUTPUT_SYSTEM_FILE} (Includes 'No Doctor' system prompt)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cd796-c67a-4676-b336-1bfddce74b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Complete.\n",
      "Total lines processed: 13405\n",
      "1. File created: medical_clean_mapped_only.jsonl (Standard format)\n",
      "2. File created: medical_with_system_prompt.jsonl (Includes 'No Doctor' system prompt)\n"
     ]
    }
   ],
   "source": [
    "process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c26c82-fcb8-4127-b504-d459aad3fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
